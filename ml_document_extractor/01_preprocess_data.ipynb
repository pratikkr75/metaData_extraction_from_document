{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/ml_document_extractor/data/train\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpcpcQQT6mQ_",
        "outputId": "4b12c0a8-93bb-4438-a1c5-44876f7c5122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18325926-Rental-Agreement-1.docx\n",
            "36199312-Rental-Agreement.png\n",
            "44737744-Maddireddy-Bhargava-Reddy-Rental-Agreement.docx\n",
            "46239065-Standard-Rental-Agreement-Rental-With-Performance-Fee.docx\n",
            "47854715-RENTAL-AGREEMENT.docx\n",
            "50070534-RENTAL-AGREEMENT.docx\n",
            "54770958-Rental-Agreement.png\n",
            "54945838-Rental-Agreement.png\n",
            "6683127-House-Rental-Contract-GERALDINE-GALINATO-v2-Page-1.docx\n",
            "6683129-House-Rental-Contract-Geraldine-Galinato-v2.docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/ml_document_extractor/data/test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCdCXvGJGImG",
        "outputId": "bf842573-0e5d-4cf3-96bb-465e6fca8aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "156155545-Rental-Agreement-Kns-Home.pdf.docx  24158401-Rental-Agreement.png\n",
            "228094620-Rental-Agreement.pdf.docx\t      95980236-Rental-Agreement.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# SCRIPT: 01_preprocess_data.py\n",
        "#\n",
        "# PURPOSE:\n",
        "#   - Read .docx and image files from the input data folder.\n",
        "#   - Perform OCR on images using Tesseract.\n",
        "#   - Merge extracted text with CSV ground truth.\n",
        "#   - Output structured JSON for training/testing.\n",
        "# ==============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "import subprocess\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Installation of Dependencies ---\n",
        "try:\n",
        "    import docx\n",
        "except ImportError:\n",
        "    print(\"Installing python-docx...\")\n",
        "    subprocess.run(['pip', 'install', '-q', 'python-docx'])\n",
        "    import docx\n",
        "\n",
        "try:\n",
        "    from PIL import Image\n",
        "except ImportError:\n",
        "    print(\"Installing Pillow...\")\n",
        "    subprocess.run(['pip', 'install', '-q', 'Pillow'])\n",
        "    from PIL import Image\n",
        "\n",
        "try:\n",
        "    import pytesseract\n",
        "except ImportError:\n",
        "    print(\"Installing pytesseract and Tesseract OCR...\")\n",
        "    subprocess.run(['pip', 'install', '-q', 'pytesseract'])\n",
        "    subprocess.run(['sudo', 'apt-get', 'install', '-y', 'tesseract-ocr'])\n",
        "    import pytesseract\n",
        "\n",
        "# --- Configuration ---\n",
        "BASE_DIR = Path('/content/drive/MyDrive/ml_document_extractor')\n",
        "DATA_DIR = BASE_DIR / 'data'\n",
        "PROCESSED_DATA_DIR = BASE_DIR / 'processed_data'\n",
        "PROCESSED_DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# --- Text Extraction Functions ---\n",
        "def extract_text_from_docx(file_path):\n",
        "    \"\"\"Extracts text from a .docx file.\"\"\"\n",
        "    try:\n",
        "        doc = docx.Document(file_path)\n",
        "        return \"\\n\".join([para.text for para in doc.paragraphs if para.text])\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading docx file {file_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_from_image(file_path):\n",
        "    \"\"\"Extracts text from an image file using OCR.\"\"\"\n",
        "    try:\n",
        "        return pytesseract.image_to_string(Image.open(file_path))\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading image file {file_path}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# --- Main Processing Function ---\n",
        "def process_files(csv_path, files_dir):\n",
        "    \"\"\"\n",
        "    Read files mentioned in CSV, extract text from .docx/.png/.jpg files,\n",
        "    and log progress or missing files.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.replace(np.nan, '', inplace=True)\n",
        "\n",
        "    processed_records = []\n",
        "\n",
        "    all_files = list(files_dir.glob(\"*\"))\n",
        "    actual_file_stems = {file.stem for file in all_files}\n",
        "    csv_filenames = set(df['File Name'].tolist())\n",
        "\n",
        "    print(f\"\\nüìÑ CSV Entries: {len(df)}\")\n",
        "    print(f\"üìÇ Folder Files: {len(actual_file_stems)}\")\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        filename = row['File Name']\n",
        "        print(f\"\\n‚û°Ô∏è  Processing: {filename}...\", end=' ')\n",
        "        context = \"\"\n",
        "        file_path = None\n",
        "        found = False\n",
        "\n",
        "        # Match based on prefix (to handle .pdf.docx, etc.)\n",
        "        matched_files = list(files_dir.glob(f\"{filename}*\"))\n",
        "        for match in matched_files:\n",
        "            suffix = match.suffix.lower()\n",
        "            file_path = match\n",
        "            found = True\n",
        "\n",
        "            if suffix == '.docx':\n",
        "                context = extract_text_from_docx(file_path)\n",
        "            elif suffix in ['.png', '.jpg', '.jpeg']:\n",
        "                context = extract_text_from_image(file_path)\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è Unsupported file type: {suffix}\")\n",
        "                context = \"\"\n",
        "            break\n",
        "\n",
        "        if not found:\n",
        "            print(\"‚ùå File not found in folder.\")\n",
        "            continue\n",
        "\n",
        "        if not context.strip():\n",
        "            print(\"‚ö†Ô∏è  No text extracted.\")\n",
        "            continue\n",
        "\n",
        "        record = {\n",
        "            'file_name': filename,\n",
        "            'context': context,\n",
        "            'ground_truth': {\n",
        "                'Agreement Value': str(row.get('Agreement Value', '')),\n",
        "                'Agreement Start Date': str(row.get('Agreement Start Date', '')),\n",
        "                'Agreement End Date': str(row.get('Agreement End Date', '')),\n",
        "                'Renewal Notice (Days)': str(row.get('Renewal Notice (Days)', '')),\n",
        "                'Party One': str(row.get('Party One', '')),\n",
        "                'Party Two': str(row.get('Party Two', ''))\n",
        "            }\n",
        "        }\n",
        "\n",
        "        processed_records.append(record)\n",
        "        print(\"‚úÖ Success.\")\n",
        "\n",
        "    # --- Log missing and extra files ---\n",
        "    missing_files = [f for f in csv_filenames if not any(Path(f).name in str(p) for p in all_files)]\n",
        "    if missing_files:\n",
        "        print(\"\\nüö´ Files listed in CSV but NOT found in folder:\")\n",
        "        for f in missing_files:\n",
        "            print(f\"  - {f}\")\n",
        "\n",
        "    extra_files = [f.stem for f in all_files if f.stem not in csv_filenames]\n",
        "    if extra_files:\n",
        "        print(\"\\nüìå Files present in folder but NOT listed in CSV:\")\n",
        "        for f in extra_files:\n",
        "            print(f\"  - {f}\")\n",
        "\n",
        "    return processed_records\n",
        "\n",
        "# --- Main Execution ---\n",
        "def main():\n",
        "    \"\"\"Main entry point to process training and testing data.\"\"\"\n",
        "    # Training\n",
        "    train_data = process_files(DATA_DIR / 'train.csv', DATA_DIR / 'train')\n",
        "    train_output_path = PROCESSED_DATA_DIR / 'train_data.json'\n",
        "    with open(train_output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(train_data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"\\n‚úÖ Training data saved to: {train_output_path}\")\n",
        "\n",
        "    # Testing\n",
        "    test_data = process_files(DATA_DIR / 'test.csv', DATA_DIR / 'test')\n",
        "    test_output_path = PROCESSED_DATA_DIR / 'test_data.json'\n",
        "    with open(test_output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(test_data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"‚úÖ Testing data saved to: {test_output_path}\")\n",
        "\n",
        "# --- Script Entry Point ---\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H52NUjU710NG",
        "outputId": "823350cf-c45a-4fa0-950b-b1dabd4984a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Installing python-docx...\n",
            "Installing pytesseract and Tesseract OCR...\n",
            "\n",
            "üìÑ CSV Entries: 10\n",
            "üìÇ Folder Files: 10\n",
            "\n",
            "‚û°Ô∏è  Processing: 6683127-House-Rental-Contract-GERALDINE-GALINATO-v2-Page-1... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 6683129-House-Rental-Contract-Geraldine-Galinato-v2... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 18325926-Rental-Agreement-1... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 24158401-Rental-Agreement... ‚ùå File not found in folder.\n",
            "\n",
            "‚û°Ô∏è  Processing: 36199312-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 44737744-Maddireddy-Bhargava-Reddy-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 47854715-RENTAL-AGREEMENT... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 50070534-RENTAL-AGREEMENT... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 54770958-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 54945838-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "üö´ Files listed in CSV but NOT found in folder:\n",
            "  - 24158401-Rental-Agreement\n",
            "\n",
            "üìå Files present in folder but NOT listed in CSV:\n",
            "  - 46239065-Standard-Rental-Agreement-Rental-With-Performance-Fee\n",
            "\n",
            "‚úÖ Training data saved to: /content/drive/MyDrive/ml_document_extractor/processed_data/train_data.json\n",
            "\n",
            "üìÑ CSV Entries: 4\n",
            "üìÇ Folder Files: 4\n",
            "\n",
            "‚û°Ô∏è  Processing: 24158401-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 95980236-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 156155545-Rental-Agreement-Kns-Home... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 228094620-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "üìå Files present in folder but NOT listed in CSV:\n",
            "  - 156155545-Rental-Agreement-Kns-Home.pdf\n",
            "  - 228094620-Rental-Agreement.pdf\n",
            "‚úÖ Testing data saved to: /content/drive/MyDrive/ml_document_extractor/processed_data/test_data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZIOQy4aiDs8",
        "outputId": "0a75770b-a157-48df-a45d-9aaa6d97e76a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================== #\n",
        "# SCRIPT: 01_combined_preprocess_and_normalize.py\n",
        "# PURPOSE:\n",
        "#   - Extracts and normalizes both context and ground_truth for train/test data.\n",
        "#   - Supports .docx and image (png/jpg) files.\n",
        "#   - Prepares clean training-ready .json format.\n",
        "# ============================================================================== #\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "import subprocess\n",
        "import re\n",
        "from dateutil.parser import parse as date_parse\n",
        "from num2words import num2words\n",
        "\n",
        "# --- Mount Google Drive (Colab only) ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Install dependencies if missing ---\n",
        "def safe_import(module, pip_name=None, apt_name=None):\n",
        "    try:\n",
        "        return __import__(module)\n",
        "    except ImportError:\n",
        "        if pip_name:\n",
        "            subprocess.run(['pip', 'install', '-q', pip_name])\n",
        "        if apt_name:\n",
        "            subprocess.run(['sudo', 'apt-get', 'install', '-y', apt_name])\n",
        "        return __import__(module)\n",
        "\n",
        "docx = safe_import(\"docx\", pip_name=\"python-docx\")\n",
        "Image = safe_import(\"PIL\", pip_name=\"Pillow\").Image\n",
        "pytesseract = safe_import(\"pytesseract\", pip_name=\"pytesseract\", apt_name=\"tesseract-ocr\")\n",
        "\n",
        "# --- Path Configuration ---\n",
        "BASE_DIR = Path('/content/drive/MyDrive/ml_document_extractor')\n",
        "DATA_DIR = BASE_DIR / 'data'\n",
        "PROCESSED_DATA_DIR = BASE_DIR / 'processed_data'\n",
        "PROCESSED_DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# --- Normalization Utilities ---\n",
        "def normalize_date(raw_date: str) -> str:\n",
        "    try:\n",
        "        dt = date_parse(raw_date, dayfirst=True)\n",
        "        return dt.strftime('%d %B %Y').lstrip(\"0\")\n",
        "    except:\n",
        "        return raw_date\n",
        "\n",
        "def normalize_amount(value: str) -> list:\n",
        "    try:\n",
        "        float_val = float(str(value).replace(\",\", \"\").strip())\n",
        "        return [\n",
        "            str(int(float_val)),\n",
        "            f\"{float_val:,.2f}\",\n",
        "            num2words(float_val, to='cardinal', lang='en').replace('-', ' ').title()\n",
        "        ]\n",
        "    except:\n",
        "        return [value]\n",
        "\n",
        "def normalize_name(name: str) -> str:\n",
        "    return re.sub(r'\\s+', ' ', name).strip().title()\n",
        "\n",
        "def normalize_field(label: str, value: str):\n",
        "    if not value.strip():\n",
        "        return value\n",
        "    if \"Date\" in label:\n",
        "        return normalize_date(value)\n",
        "    elif \"Value\" in label:\n",
        "        return normalize_amount(value)\n",
        "    elif \"Party\" in label:\n",
        "        return normalize_name(value)\n",
        "    elif \"Renewal Notice\" in label:\n",
        "        try:\n",
        "            int_val = int(float(value.strip()))\n",
        "            return str(int_val)\n",
        "        except:\n",
        "            return value.strip()\n",
        "    return value\n",
        "\n",
        "# --- Preprocessing Context ---\n",
        "def preprocess_context(text: str) -> str:\n",
        "    text = re.sub(r'\\s+', ' ', text)  # normalize spacing\n",
        "    return text.strip()\n",
        "\n",
        "# --- Text Extraction ---\n",
        "def extract_text_from_docx(file_path):\n",
        "    try:\n",
        "        doc = docx.Document(file_path)\n",
        "        return \"\\n\".join([para.text for para in doc.paragraphs if para.text])\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading docx: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_from_image(file_path):\n",
        "    try:\n",
        "        return pytesseract.image_to_string(Image.open(file_path))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading image: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# --- Main Processor ---\n",
        "def process_files(csv_path, files_dir):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.replace(np.nan, '', inplace=True)\n",
        "\n",
        "    processed_records = []\n",
        "    failed_contexts = []\n",
        "\n",
        "    all_files = list(files_dir.glob(\"*\"))\n",
        "    actual_file_stems = {file.stem for file in all_files}\n",
        "    csv_filenames = set(df['File Name'].tolist())\n",
        "\n",
        "    print(f\"\\nüìÑ CSV Entries: {len(df)} | üìÇ Files in folder: {len(all_files)}\")\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        filename = row['File Name']\n",
        "        print(f\"\\n‚û°Ô∏è  Processing: {filename}...\", end=' ')\n",
        "        context = \"\"\n",
        "        file_path = None\n",
        "\n",
        "        matched_files = list(files_dir.glob(f\"{filename}*\"))\n",
        "        if not matched_files:\n",
        "            print(\"‚ùå File not found.\")\n",
        "            continue\n",
        "\n",
        "        file_path = matched_files[0]\n",
        "        suffix = file_path.suffix.lower()\n",
        "\n",
        "        if suffix == '.docx':\n",
        "            context = extract_text_from_docx(file_path)\n",
        "        elif suffix in ['.png', '.jpg', '.jpeg']:\n",
        "            context = extract_text_from_image(file_path)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Unsupported file type: {suffix}\")\n",
        "            continue\n",
        "\n",
        "        if not context.strip():\n",
        "            print(f\"‚ö†Ô∏è No text extracted for: {file_path.name}\")\n",
        "            failed_contexts.append(file_path.name)\n",
        "            continue\n",
        "\n",
        "        context = preprocess_context(context)\n",
        "\n",
        "        gt = {}\n",
        "        for label in ['Agreement Value', 'Agreement Start Date', 'Agreement End Date',\n",
        "                      'Renewal Notice (Days)', 'Party One', 'Party Two']:\n",
        "            val = str(row.get(label, ''))\n",
        "            norm_val = normalize_field(label, val)\n",
        "            gt[label] = norm_val\n",
        "\n",
        "        record = {\n",
        "            'file_name': filename,\n",
        "            'context': context,\n",
        "            'ground_truth': gt\n",
        "        }\n",
        "        processed_records.append(record)\n",
        "        print(\"‚úÖ Success.\")\n",
        "\n",
        "    missing_files = [f for f in csv_filenames if f not in actual_file_stems]\n",
        "    if missing_files:\n",
        "        print(\"\\nüö´ Missing Files from folder:\")\n",
        "        for f in missing_files:\n",
        "            print(f\"  - {f}\")\n",
        "\n",
        "    extra_files = [f.stem for f in all_files if f.stem not in csv_filenames]\n",
        "    if extra_files:\n",
        "        print(\"\\nüìå Extra Files in folder not listed in CSV:\")\n",
        "        for f in extra_files:\n",
        "            print(f\"  - {f}\")\n",
        "\n",
        "    if failed_contexts:\n",
        "        print(\"\\n‚ö†Ô∏è Files skipped due to empty context:\")\n",
        "        for f in failed_contexts:\n",
        "            print(f\"  - {f}\")\n",
        "\n",
        "    return processed_records\n",
        "\n",
        "# --- Main ---\n",
        "def main():\n",
        "    train_data = process_files(DATA_DIR / 'train.csv', DATA_DIR / 'train')\n",
        "    train_output_path = PROCESSED_DATA_DIR / 'train_data.json'\n",
        "    with open(train_output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(train_data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"\\n‚úÖ train_data.json saved!\")\n",
        "\n",
        "    test_data = process_files(DATA_DIR / 'test.csv', DATA_DIR / 'test')\n",
        "    test_output_path = PROCESSED_DATA_DIR / 'test_data.json'\n",
        "    with open(test_output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(test_data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"‚úÖ test_data.json saved!\")\n",
        "\n",
        "# --- Entry Point ---\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "Esn1dRKt4leY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b244d1d-2f8e-4b51-c944-9bb1ec939fd4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "üìÑ CSV Entries: 10 | üìÇ Files in folder: 10\n",
            "\n",
            "‚û°Ô∏è  Processing: 6683127-House-Rental-Contract-GERALDINE-GALINATO-v2-Page-1... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 6683129-House-Rental-Contract-Geraldine-Galinato-v2... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 18325926-Rental-Agreement-1... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 24158401-Rental-Agreement... ‚ùå File not found.\n",
            "\n",
            "‚û°Ô∏è  Processing: 36199312-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 44737744-Maddireddy-Bhargava-Reddy-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 47854715-RENTAL-AGREEMENT... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 50070534-RENTAL-AGREEMENT... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 54770958-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 54945838-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "üö´ Missing Files from folder:\n",
            "  - 24158401-Rental-Agreement\n",
            "\n",
            "üìå Extra Files in folder not listed in CSV:\n",
            "  - 46239065-Standard-Rental-Agreement-Rental-With-Performance-Fee\n",
            "\n",
            "‚úÖ train_data.json saved!\n",
            "\n",
            "üìÑ CSV Entries: 4 | üìÇ Files in folder: 4\n",
            "\n",
            "‚û°Ô∏è  Processing: 24158401-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 95980236-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 156155545-Rental-Agreement-Kns-Home... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 228094620-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "üö´ Missing Files from folder:\n",
            "  - 228094620-Rental-Agreement\n",
            "  - 156155545-Rental-Agreement-Kns-Home\n",
            "\n",
            "üìå Extra Files in folder not listed in CSV:\n",
            "  - 156155545-Rental-Agreement-Kns-Home.pdf\n",
            "  - 228094620-Rental-Agreement.pdf\n",
            "‚úÖ test_data.json saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================== #\n",
        "# SCRIPT: 01_combined_preprocess_and_normalize.py\n",
        "# PURPOSE:\n",
        "#   - Extracts and normalizes both context and ground_truth for train/test data.\n",
        "#   - Supports .docx and image (png/jpg) files.\n",
        "#   - Prepares clean training-ready .json format.\n",
        "# ============================================================================== #\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import json\n",
        "import subprocess\n",
        "import re\n",
        "from dateutil.parser import parse as date_parse\n",
        "from num2words import num2words\n",
        "\n",
        "# --- Mount Google Drive (Colab only) ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Install dependencies if missing ---\n",
        "def safe_import(module, pip_name=None, apt_name=None):\n",
        "    try:\n",
        "        return __import__(module)\n",
        "    except ImportError:\n",
        "        if pip_name:\n",
        "            subprocess.run(['pip', 'install', '-q', pip_name])\n",
        "        if apt_name:\n",
        "            subprocess.run(['sudo', 'apt-get', 'install', '-y', apt_name])\n",
        "        return __import__(module)\n",
        "\n",
        "docx = safe_import(\"docx\", pip_name=\"python-docx\")\n",
        "Image = safe_import(\"PIL\", pip_name=\"Pillow\").Image\n",
        "pytesseract = safe_import(\"pytesseract\", pip_name=\"pytesseract\", apt_name=\"tesseract-ocr\")\n",
        "\n",
        "# --- Path Configuration ---\n",
        "BASE_DIR = Path('/content/drive/MyDrive/ml_document_extractor')\n",
        "DATA_DIR = BASE_DIR / 'data'\n",
        "PROCESSED_DATA_DIR = BASE_DIR / 'processed_data'\n",
        "PROCESSED_DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# --- Normalization Utilities ---\n",
        "def normalize_date(raw_date: str) -> str:\n",
        "    try:\n",
        "        dt = date_parse(raw_date, dayfirst=True, fuzzy=True)\n",
        "        return dt.strftime('%d %B %Y')\n",
        "    except:\n",
        "        return raw_date.strip()\n",
        "\n",
        "def normalize_amount(value: str) -> list:\n",
        "    try:\n",
        "        float_val = float(str(value).replace(\",\", \"\").strip())\n",
        "        return [\n",
        "            str(int(float_val)),\n",
        "            f\"{float_val:,.2f}\",\n",
        "            num2words(float_val, to='cardinal', lang='en').replace('-', ' ').title()\n",
        "        ]\n",
        "    except:\n",
        "        return [value.strip()]\n",
        "\n",
        "def normalize_name(name: str) -> str:\n",
        "    return re.sub(r'\\s+', ' ', name).strip().title()\n",
        "\n",
        "def normalize_field(label: str, value: str):\n",
        "    value = value.strip()\n",
        "    if not value:\n",
        "        return value\n",
        "    if \"Date\" in label:\n",
        "        return normalize_date(value)\n",
        "    elif \"Value\" in label:\n",
        "        return normalize_amount(value)\n",
        "    elif \"Party\" in label:\n",
        "        return normalize_name(value)\n",
        "    elif \"Renewal Notice\" in label:\n",
        "        try:\n",
        "            return str(int(float(value)))\n",
        "        except:\n",
        "            if \"month\" in value.lower():\n",
        "                match = re.search(r'(\\d+|one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve)', value, re.I)\n",
        "                if match:\n",
        "                    word = match.group(0).lower()\n",
        "                    word_to_number = {\n",
        "                        \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5,\n",
        "                        \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9, \"ten\": 10,\n",
        "                        \"eleven\": 11, \"twelve\": 12\n",
        "                    }\n",
        "                    return str(word_to_number.get(word, word))\n",
        "            return value\n",
        "    return value\n",
        "\n",
        "# --- Preprocessing Context ---\n",
        "def preprocess_context(text: str) -> str:\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'(\\d{1,2})(st|nd|rd|th)?\\s+(January|February|March|April|May|June|July|August|September|October|November|December)',\n",
        "                  lambda m: f\"{int(m.group(1)):02d} {m.group(3)}\", text, flags=re.I)\n",
        "    text = re.sub(r'(?i)(one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve)\\s+months?',\n",
        "                  lambda m: str({\n",
        "                      \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4, \"five\": 5,\n",
        "                      \"six\": 6, \"seven\": 7, \"eight\": 8, \"nine\": 9,\n",
        "                      \"ten\": 10, \"eleven\": 11, \"twelve\": 12\n",
        "                  }[m.group(1).lower()]), text)\n",
        "    text = re.sub(r'(\\d+)\\s+months?', r'\\1', text)\n",
        "    return text.strip()\n",
        "\n",
        "# --- Text Extraction ---\n",
        "def extract_text_from_docx(file_path):\n",
        "    try:\n",
        "        doc = docx.Document(file_path)\n",
        "        return \"\\n\".join([para.text for para in doc.paragraphs if para.text])\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading docx: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_text_from_image(file_path):\n",
        "    try:\n",
        "        return pytesseract.image_to_string(Image.open(file_path))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading image: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# --- Main Processor ---\n",
        "def process_files(csv_path, files_dir):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df.replace(np.nan, '', inplace=True)\n",
        "\n",
        "    processed_records = []\n",
        "    failed_contexts = []\n",
        "\n",
        "    all_files = list(files_dir.glob(\"*\"))\n",
        "    actual_file_stems = {file.stem for file in all_files}\n",
        "    csv_filenames = set(df['File Name'].tolist())\n",
        "\n",
        "    print(f\"\\nüìÑ CSV Entries: {len(df)} | üìÇ Files in folder: {len(all_files)}\")\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        filename = row['File Name']\n",
        "        print(f\"\\n‚û°Ô∏è  Processing: {filename}...\", end=' ')\n",
        "        context = \"\"\n",
        "\n",
        "        matched_files = list(files_dir.glob(f\"{filename}*\"))\n",
        "        if not matched_files:\n",
        "            print(\"‚ùå File not found.\")\n",
        "            continue\n",
        "\n",
        "        file_path = matched_files[0]\n",
        "        suffix = file_path.suffix.lower()\n",
        "\n",
        "        if suffix == '.docx':\n",
        "            context = extract_text_from_docx(file_path)\n",
        "        elif suffix in ['.png', '.jpg', '.jpeg']:\n",
        "            context = extract_text_from_image(file_path)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Unsupported file type: {suffix}\")\n",
        "            continue\n",
        "\n",
        "        if not context.strip():\n",
        "            print(f\"‚ö†Ô∏è No text extracted for: {file_path.name}\")\n",
        "            failed_contexts.append(file_path.name)\n",
        "            continue\n",
        "\n",
        "        context = preprocess_context(context)\n",
        "\n",
        "        gt = {}\n",
        "        for label in ['Agreement Value', 'Agreement Start Date', 'Agreement End Date',\n",
        "                      'Renewal Notice (Days)', 'Party One', 'Party Two']:\n",
        "            val = str(row.get(label, ''))\n",
        "            norm_val = normalize_field(label, val)\n",
        "            gt[label] = norm_val\n",
        "\n",
        "        record = {\n",
        "            'file_name': filename,\n",
        "            'context': context,\n",
        "            'ground_truth': gt\n",
        "        }\n",
        "        processed_records.append(record)\n",
        "        print(\"‚úÖ Success.\")\n",
        "\n",
        "    # Log extra/missing files\n",
        "    missing_files = [f for f in csv_filenames if f not in actual_file_stems]\n",
        "    if missing_files:\n",
        "        print(\"\\nüö´ Missing Files from folder:\")\n",
        "        for f in missing_files:\n",
        "            print(f\"  - {f}\")\n",
        "\n",
        "    extra_files = [f.stem for f in all_files if f.stem not in csv_filenames]\n",
        "    if extra_files:\n",
        "        print(\"\\nüìå Extra Files in folder not listed in CSV:\")\n",
        "        for f in extra_files:\n",
        "            print(f\"  - {f}\")\n",
        "\n",
        "    if failed_contexts:\n",
        "        print(\"\\n‚ö†Ô∏è Files skipped due to empty context:\")\n",
        "        for f in failed_contexts:\n",
        "            print(f\"  - {f}\")\n",
        "\n",
        "    return processed_records\n",
        "\n",
        "# --- Main ---\n",
        "def main():\n",
        "    train_data = process_files(DATA_DIR / 'train.csv', DATA_DIR / 'train')\n",
        "    with open(PROCESSED_DATA_DIR / 'train_data.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(train_data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"\\n‚úÖ train_data.json saved!\")\n",
        "\n",
        "    test_data = process_files(DATA_DIR / 'test.csv', DATA_DIR / 'test')\n",
        "    with open(PROCESSED_DATA_DIR / 'test_data.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(test_data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"‚úÖ test_data.json saved!\")\n",
        "\n",
        "# --- Entry Point ---\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCDurs5ch9_q",
        "outputId": "be4c957b-63e4-4e87-9050-80962e39f849"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "üìÑ CSV Entries: 10 | üìÇ Files in folder: 10\n",
            "\n",
            "‚û°Ô∏è  Processing: 6683127-House-Rental-Contract-GERALDINE-GALINATO-v2-Page-1... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 6683129-House-Rental-Contract-Geraldine-Galinato-v2... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 18325926-Rental-Agreement-1... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 24158401-Rental-Agreement... ‚ùå File not found.\n",
            "\n",
            "‚û°Ô∏è  Processing: 36199312-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 44737744-Maddireddy-Bhargava-Reddy-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 47854715-RENTAL-AGREEMENT... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 50070534-RENTAL-AGREEMENT... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 54770958-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 54945838-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "üö´ Missing Files from folder:\n",
            "  - 24158401-Rental-Agreement\n",
            "\n",
            "üìå Extra Files in folder not listed in CSV:\n",
            "  - 46239065-Standard-Rental-Agreement-Rental-With-Performance-Fee\n",
            "\n",
            "‚úÖ train_data.json saved!\n",
            "\n",
            "üìÑ CSV Entries: 4 | üìÇ Files in folder: 4\n",
            "\n",
            "‚û°Ô∏è  Processing: 24158401-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 95980236-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 156155545-Rental-Agreement-Kns-Home... ‚úÖ Success.\n",
            "\n",
            "‚û°Ô∏è  Processing: 228094620-Rental-Agreement... ‚úÖ Success.\n",
            "\n",
            "üö´ Missing Files from folder:\n",
            "  - 228094620-Rental-Agreement\n",
            "  - 156155545-Rental-Agreement-Kns-Home\n",
            "\n",
            "üìå Extra Files in folder not listed in CSV:\n",
            "  - 156155545-Rental-Agreement-Kns-Home.pdf\n",
            "  - 228094620-Rental-Agreement.pdf\n",
            "‚úÖ test_data.json saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WDucTQSGxVkv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}